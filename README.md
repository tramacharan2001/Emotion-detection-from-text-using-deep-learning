Emotion detection algorithms are having a large demand in the current market as companies use them to get the result from customer reviews so that they can improve their product as well as personalize their product to improve the experience with their product. For perfoming this task we use various algoriths like LSTM, GRU. In this current project I am using LSTM algorithm to create a model.

Long Short-Term Memory (LSTM) networks are a type of recurrent neural network capable of learning order dependence in sequence prediction problems.

This is a behavior required in complex problem domains like machine translation, speech recognition, and more. LSTMs are a complex area of deep learning. It can be hard to get your hands around what LSTMs are, and how terms like bidirectional and sequence-to-sequence relate to the field.

The success of LSTMs is in their claim to be one of the first implements to overcome the technical problems and deliver on the promise of recurrent neural networks. The two technical problems overcome by LSTMs are vanishing gradients and exploding gradients, both related to how the network is trained.

The key to the LSTM solution to the technical problems was the specific internal structure of the units used in the model. Rather than go into the equations that govern how LSTMs are fit, analogy is a useful tool to quickly get a handle on how they work.

Multiple analogies can help to give purchase on what differentiates LSTMs from traditional neural networks comprised of simple neurons. It is important to get a handle on exactly what type of sequence learning problems that LSTMs are suitable to address.
